#  STEP 1: Install required libraries
# Uninstall existing torch and torchvision to avoid conflicts
!pip uninstall -y torch torchvision torchaudio

# Install the correct version of torch and torchvision with CUDA support (adjust the URL based on your CUDA version if needed)
# This example uses CUDA 11.8, common in many environments.
# Refer to https://pytorch.org/get-started/locally/ for the exact command for your setup.
!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118

# Install other required libraries
!pip install -q transformers diffusers gradio accelerate xformers opencv-python textstat clip

#  STEP 2: Import libraries
import gradio as gr
from transformers import pipeline, CLIPProcessor, CLIPModel
from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline, StableVideoDiffusionPipeline
import torch
import random
from typing import Tuple
import cv2
import numpy as np
from PIL import Image
import textstat

#  STEP 3: Model choices
MODEL_CHOICES = {
    "Mistral-7B (Recommended)": "mistralai/Mistral-7B-v0.1",
    "OpenChat-3.5": "openchat/openchat_3.5",
    "Zephyr-7B": "HuggingFaceH4/zephyr-7b-beta",
    "Phi-2 (Lightweight)": "microsoft/phi-2",
    "GPT-2 (Fastest)": "gpt2"
}

IMAGE_MODEL_CHOICES = {
    "Stable Diffusion XL (High Quality)": "stabilityai/stable-diffusion-xl-base-1.0",
    "Stable Diffusion v1.5 (Balanced)": "runwayml/stable-diffusion-v1-5",
    "Stable Diffusion v1.4 (Lightweight)": "CompVis/stable-diffusion-v1-4"
}

VIDEO_MODEL_CHOICES = {
    "Stable Video Diffusion (High Quality)": "stabilityai/stable-video-diffusion-img2vid",
    "Lightweight Video Model (Fallback)": "placeholder/lightweight-video-model"
}

VIDEO_STYLES = ["Cinematic", "Smooth Animation", "Dynamic", "Minimalist", "Retro"]

#  STEP 4: Initialize models with fallback system
def initialize_models(
    selected_text_model: str = "distilgpt2",
    selected_image_model: str = "stabilityai/stable-diffusion-xl-base-1.0",
    selected_video_model: str = "stabilityai/stable-video-diffusion-img2vid"
):
    try:
        text_gen = pipeline("text-generation", model=selected_text_model, device_map="auto")
        print(f"Using {selected_text_model} for text generation")
    except Exception as e:
        print(f"Error loading {selected_text_model}: {e}. Falling back to GPT-2")
        text_gen = pipeline("text-generation", model="gpt2")

    try:
        if selected_image_model == "stabilityai/stable-diffusion-xl-base-1.0":
            pipe = StableDiffusionXLPipeline.from_pretrained(
                selected_image_model, torch_dtype=torch.float16, use_safetensors=True, variant="fp16"
            )
            pipe.enable_xformers_memory_efficient_attention()
        else:
            pipe = StableDiffusionPipeline.from_pretrained(
                selected_image_model, torch_dtype=torch.float16, use_auth_token=False
            )
        pipe = pipe.to("cuda")
        print(f"Using {selected_image_model} for image generation")
    except Exception as e:
        print(f"Error loading {selected_image_model}: {e}. Falling back to Stable Diffusion v1.5")
        pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_auth_token=False
        ).to("cuda")

    try:
        if selected_video_model == "stabilityai/stable-video-diffusion-img2vid":
            video_pipe = StableVideoDiffusionPipeline.from_pretrained(
                selected_video_model, torch_dtype=torch.float16, use_safetensors=True
            )
            video_pipe.enable_xformers_memory_efficient_attention()
            video_pipe = video_pipe.to("cuda")
            print(f"Using {selected_video_model} for video generation")
        else:
            video_pipe = StableDiffusionPipeline.from_pretrained(
                "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_auth_token=False
            ).to("cuda")
            print(f"Using fallback lightweight video model")
    except Exception as e:
        print(f"Error loading {selected_video_model}: {e}. Falling back to Stable Diffusion v1.5 (image-based)")
        video_pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_auth_token=False
        ).to("cuda")

    return text_gen, pipe, video_pipe

#  STEP 5: Evaluation metrics
def compute_metrics(text: str, image: Image.Image, video_path: str, prompt: str, tone: str) -> dict:
    """Compute evaluation metrics for text, image, and video"""
    metrics = {"text": {}, "image": {}, "video": {}, "total_score": 0, "grade": "F"}

    # Text metrics
    try:
        coherence = textstat.flesch_reading_ease(text)
        coherence = min(max(coherence, 0), 100)
        metrics["text"]["coherence"] = coherence

        sentiment_analyzer = pipeline("sentiment-analysis")
        sentiment = sentiment_analyzer(text)[0]
        tone_score = 0
        if tone.lower() == "exciting" and sentiment["label"] == "POSITIVE":
            tone_score = sentiment["score"] * 100
        elif tone.lower() in ["professional", "inspirational"] and sentiment["label"] == "POSITIVE":
            tone_score = sentiment["score"] * 80
        elif tone.lower() == "friendly" and sentiment["label"] == "POSITIVE":
            tone_score = sentiment["score"] * 90
        metrics["text"]["tone_alignment"] = tone_score
    except Exception as e:
        print(f"Text metrics error: {e}")
        metrics["text"] = {"coherence": 0, "tone_alignment": 0}

    # Image metrics
    try:
        if image:
            clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            inputs = clip_processor(text=[prompt], images=image, return_tensors="pt", padding=True)
            outputs = clip_model(**inputs)
            clip_score = outputs.logits_per_image.softmax(dim=1)[0][0].item() * 100
            metrics["image"]["clip_score"] = clip_score

            img_array = np.array(image.convert("L"))
            laplacian_var = cv2.Laplacian(img_array, cv2.CV_64F).var()
            sharpness = min(laplacian_var / 100, 100)
            metrics["image"]["sharpness"] = sharpness
        else:
            metrics["image"] = {"clip_score": 0, "sharpness": 0}
    except Exception as e:
        print(f"Image metrics error: {e}")
        metrics["image"] = {"clip_score": 0, "sharpness": 0}

    # Video metrics
    try:
        if video_path:
            cap = cv2.VideoCapture(video_path)
            frames = []
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            cap.release()

            if len(frames) > 1:
                mse_scores = []
                for i in range(len(frames) - 1):
                    mse = np.mean((frames[i] - frames[i + 1]) ** 2)
                    mse_scores.append(mse)
                consistency = max(100 - np.mean(mse_scores) / 100, 0)
            else:
                consistency = 0
            metrics["video"]["frame_consistency"] = consistency

            clip_scores = []
            for frame in frames[:5]:
                frame_img = Image.fromarray(frame)
                inputs = clip_processor(text=[prompt], images=frame_img, return_tensors="pt", padding=True)
                outputs = clip_model(**inputs)
                clip_scores.append(outputs.logits_per_image.softmax(dim=1)[0][0].item() * 100)
            metrics["video"]["clip_score"] = np.mean(clip_scores) if clip_scores else 0
        else:
            metrics["video"] = {"frame_consistency": 0, "clip_score": 0}
    except Exception as e:
        print(f"Video metrics error: {e}")
        metrics["video"] = {"frame_consistency": 0, "clip_score": 0}

    # Compute total score (weighted average)
    text_score = (metrics["text"]["coherence"] + metrics["text"]["tone_alignment"]) / 2
    image_score = (metrics["image"]["clip_score"] + metrics["image"]["sharpness"]) / 2
    video_score = (metrics["video"]["frame_consistency"] + metrics["video"]["clip_score"]) / 2
    total_score = (text_score * 0.3 + image_score * 0.4 + video_score * 0.3)
    metrics["total_score"] = total_score

    # Assign grade
    if total_score >= 90:
        metrics["grade"] = "A"
    elif total_score >= 80:
        metrics["grade"] = "B"
    elif total_score >= 70:
        metrics["grade"] = "C"
    elif total_score >= 60:
        metrics["grade"] = "D"
    else:
        metrics["grade"] = "F"

    return metrics

#  STEP 6: Enhanced template system
TEMPLATES = {
    "Tech Launch": "üåü Introducing: {} üöÄ\nExperience the future with our AI-powered solution. Try it now!",
    "Bold Sale": "üî• LIMITED TIME OFFER: {} üî•\nGrab it before it's gone!",
    "Tips & Tricks": "üí° Did you know? {}\nStart using this today and boost your productivity!",
    "Inspiring": "‚ú® Imagine a world where {} ‚ú®\nWe're making that a reality.",
    "LinkedIn Professional": "üöÄ Excited to share: {}\nThis innovation is transforming our industry. What are your thoughts?",
    "Instagram Caption": "üåà Just created something amazing: {}\nDouble tap if you agree! ‚ù§Ô∏è",
    "Product Ad": "üõçÔ∏è Meet your new favorite: {}\nNow available with exclusive launch discounts!",
    "Twitter Thread": "üßµ Thread: The truth about {}\n1/ Most people don't realize...",
    "Facebook Post": "üì¢ Big News: {}\nTag someone who needs to see this!",
    "YouTube Description": "In this post, we explore: {}\n\nLike & Subscribe for more content!",
    "Pinterest Pin": "üìå Save this idea: {}\nPerfect for when you need inspiration!",
    "TikTok Challenge": "üéµ New challenge alert: {}\nDuet this and show us your version!",
    "Email Newsletter": "üíå In today's edition: {}\nRead more below and don't miss our special offer!",
    "Blog Header": "üìù The Ultimate Guide to {}\nEverything you need to know in one place",
}

#  STEP 7: Hashtag and style configurations
HASHTAGS = {
    "Tech": ["#Tech", "#Innovation", "#AI", "#FutureTech", "#DigitalTransformation"],
    "Business": ["#Business", "#Entrepreneur", "#Startup", "#Leadership", "#Success"],
    "Lifestyle": ["#Lifestyle", "#Inspiration", "#Motivation", "#SelfCare", "#Mindset"],
    "Creative": ["#Creative", "#Design", "#Art", "#Photography", "#CreateEveryday"],
    "General": ["#Trending", "#Viral", "#MustSee", "#DidYouKnow", "#GameChanger"]
}

IMAGE_STYLES = [
    "Realistic", "Digital Art", "Minimalist", "3D Render",
    "Watercolor", "Cyberpunk", "Neon", "Vintage",
    "Futuristic", "Cartoon", "Anime", "Oil Painting"
]

#  STEP 8: Enhanced generation function
def generate_social_post(
    prompt: str,
    template_type: str,
    platform: str,
    tone: str,
    include_hashtags: bool,
    hashtag_category: str,
    image_style: str,
    image_quality: str,
    additional_info: str,
    selected_text_model: str,
    selected_image_model: str,
    selected_video_model: str,
    num_frames: int,
    seconds: float,
    motion_intensity: str,
    video_style: str
) -> Tuple[str, gr.Image, gr.Video, str]:
    """Generate social media content with text, image, and video, and compute metrics"""
    text_gen, image_pipe, video_pipe = initialize_models(
        selected_text_model, selected_image_model, selected_video_model
    )

    # 1. Template processing
    template = TEMPLATES.get(template_type, "{}")
    final_prompt = template.format(prompt)
    if additional_info:
        final_prompt += f"\n\nAdditional context: {additional_info}"

    # 2. Text generation
    tone_prompt = f"Write this in a {tone.lower()} tone: {final_prompt}"
    for attempt in range(2):
        try:
            gen_text = text_gen(
                tone_prompt,
                max_length=150,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True
            )[0]['generated_text']
            break
        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            if attempt == 1:
                gen_text = "Content generation failed (OOM). Please try a shorter prompt."

    # 3. Hashtag generation
    if include_hashtags:
        base_tags = HASHTAGS.get(hashtag_category, HASHTAGS["General"])
        custom_tags = [f"#{word}" for word in prompt.replace(',', '').split()[:3] if len(word) > 3]
        all_tags = list(set(base_tags + custom_tags))[:5]
        random.shuffle(all_tags)
        gen_text += "\n\n" + " ".join(all_tags)

    # 4. Image generation
    image_prompt = f"{image_style} style, {prompt}, high {image_quality} quality"
    if image_style == "Realistic":
        image_prompt += ", photorealistic, 8k, detailed"
    elif image_style == "Minimalist":
        image_prompt += ", simple background, clean lines, negative space"

    is_xl = isinstance(image_pipe, StableDiffusionXLPipeline)
    guidance_scale = 7.5 if is_xl else (9 if image_quality == "detailed" else 7)
    steps = 40 if is_xl else (50 if image_quality == "detailed" else 30)

    try:
        image = image_pipe(
            image_prompt,
            guidance_scale=guidance_scale,
            num_inference_steps=steps,
            height=1024 if is_xl else 512,
            width=1024 if is_xl else 512
        ).images[0]
        image_path = f"generated_image_{random.randint(1000, 9999)}.png"
        image.save(image_path)
        print(f"Image saved at: {image_path}")
    except Exception as e:
        print(f"Image generation error: {e}")
        image = None
        image_path = None

    # 5. Video generation with changing frames
    video = None
    try:
        fps = num_frames / max(seconds, 0.1)
        video_prompt = f"{video_style} style, {prompt}, high quality"
        motion_scale = {"Low": 0.5, "Medium": 1.0, "High": 1.5}.get(motion_intensity, 1.0)

        if video_style == "Cinematic":
            video_prompt += ", dramatic lighting, epic composition"
        elif video_style == "Minimalist":
            video_prompt += ", simple transitions, clean visuals"
        elif video_style == "Retro":
            video_prompt += ", vintage film grain, 80s aesthetic"

        if isinstance(video_pipe, StableVideoDiffusionPipeline):
            # Generate video starting from the generated image
            if image:
                video_frames = video_pipe(
                    image=image,  # Use initial image
                    num_frames=num_frames,
                    guidance_scale=guidance_scale,
                    num_inference_steps=steps,
                    height=512,
                    width=512,
                    motion_bucket_id=int(motion_scale * 127),
                    fps=int(fps)
                ).frames
            else:
                video_frames = video_pipe(
                    video_prompt,
                    num_frames=num_frames,
                    guidance_scale=guidance_scale,
                    num_inference_steps=steps,
                    height=512,
                    width=512,
                    motion_bucket_id=int(motion_scale * 127),
                    fps=int(fps)
                ).frames
            out_file = f"generated_video_{random.randint(1000, 9999)}.mp4"
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(out_file, fourcc, fps, (512, 512))
            for frame in video_frames:
                frame = (frame * 255).astype(np.uint8)
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                out.write(frame)
            out.release()
            video = out_file
            print(f"Video saved at: {video}")
        else:
            # Fallback: Generate multiple images for frames
            video_frames = []
            for i in range(num_frames):
                frame_prompt = f"{video_prompt}, frame {i+1}, slight variation"
                try:
                    frame_image = image_pipe(
                        frame_prompt,
                        guidance_scale=guidance_scale,
                        num_inference_steps=steps,
                        height=512,
                        width=512
                    ).images[0]
                    video_frames.append(np.array(frame_image))
                except Exception as e:
                    print(f"Frame {i+1} generation error: {e}")
                    if image:
                        video_frames.append(np.array(image))
                    else:
                        video_frames.append(np.zeros((512, 512, 3), dtype=np.uint8))

            out_file = f"fallback_video_{random.randint(1000, 9999)}.mp4"
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(out_file, fourcc, fps, (512, 512))
            for frame in video_frames:
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                out.write(frame)
            out.release()
            video = out_file
            print(f"Fallback video with multiple frames saved at: {video}")
    except Exception as e:
        print(f"Video generation error: {e}")
        video = None

    # 6. Platform-specific formatting
    if platform == "Instagram":
        gen_text = "üì∏ " + gen_text
    elif platform == "Twitter":
        gen_text = gen_text[:280]
    elif platform == "LinkedIn":
        gen_text = gen_text.replace("üöÄ", "üîπ")

    # 7. Print results
    print("\n=== Generated Results ===")
    print(f"Text:\n{gen_text}")
    print(f"Image Path: {image_path if image_path else 'None'}")
    print(f"Video Path: {video if video else 'None'}")

    # 8. Compute and print metrics
    metrics = compute_metrics(gen_text, image, video, prompt, tone)
    metrics_text = (
        f"\n=== Evaluation Metrics ===\n"
        f"Text Metrics:\n"
        f"  Coherence: {metrics['text']['coherence']:.2f}/100\n"
        f"  Tone Alignment: {metrics['text']['tone_alignment']:.2f}/100\n"
        f"Image Metrics:\n"
        f"  CLIP Score: {metrics['image']['clip_score']:.2f}/100\n"
        f"  Sharpness: {metrics['image']['sharpness']:.2f}/100\n"
        f"Video Metrics:\n"
        f"  Frame Consistency: {metrics['video']['frame_consistency']:.2f}/100\n"
        f"  CLIP Score: {metrics['video']['clip_score']:.2f}/100\n"
        f"Total Score: {metrics['total_score']:.2f}/100\n"
        f"Grade: {metrics['grade']}"
    )
    print(metrics_text)

    return gen_text, image, video, metrics_text

#  STEP 9: Enhanced Gradio Interface
with gr.Blocks(
    title="AI Social Media Content Generator Pro",
    theme=gr.themes.Soft(primary_hue="blue", secondary_hue="teal", font=[gr.themes.GoogleFont("Poppins")]),
    css="""
    .gradio-container { max-width: 1200px !important; }
    .title { text-align: center; font-weight: 700 !important; }
    .input-section { background: #f9fafb; padding: 20px; border-radius: 10px; }
    .output-section { background: #ffffff; padding: 20px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
    .generate-btn { background: linear-gradient(45deg, #4f46e5, #06b6d4) !important; border: none !important; }
    .generate-btn:hover { transform: scale(1.02); transition: transform 0.2s; }
    """
) as app:

    gr.Markdown("""
    <div style='text-align: center'>
        <h1 style='font-weight: 800; color: #4f46e5'>AI Social Media Content Generator Pro</h1>
        <p style='color: #64748b'>Create stunning text, images, and videos with advanced AI models</p>
    </div>
    """)

    with gr.Row():
        with gr.Column(scale=1, elem_classes="input-section"):
            gr.Markdown("### üéØ Content Parameters")
            prompt = gr.Textbox(label="‚ú® Your Idea", placeholder="e.g., AI document summarizer", lines=2)
            with gr.Row():
                template_type = gr.Dropdown(list(TEMPLATES.keys()), label="üé® Template", value="Tech Launch")
                platform = gr.Dropdown(
                    ["All Platforms", "LinkedIn", "Instagram", "Twitter", "Facebook"], label="üì± Platform", value="All Platforms"
                )
            tone = gr.Dropdown(
                ["Professional", "Friendly", "Exciting", "Inspirational"], label="üé≠ Tone", value="Professional"
            )
            selected_text_model = gr.Dropdown(
                list(MODEL_CHOICES.keys()), label="ü§ñ Text Generation Model", value="Mistral-7B (Recommended)"
            )
            selected_image_model = gr.Dropdown(
                list(IMAGE_MODEL_CHOICES.keys()), label="üñºÔ∏è Image Generation Model", value="Stable Diffusion XL (High Quality)"
            )
            selected_video_model = gr.Dropdown(
                list(VIDEO_MODEL_CHOICES.keys()), label="üé• Video Generation Model", value="Stable Video Diffusion (High Quality)"
            )
            with gr.Accordion("‚öôÔ∏è Advanced Settings", open=False):
                additional_info = gr.Textbox(label="‚ÑπÔ∏è Additional Context", lines=2)
                with gr.Row():
                    include_hashtags = gr.Checkbox(label="Add Hashtags", value=True)
                    hashtag_category = gr.Dropdown(list(HASHTAGS.keys()), label="Category", value="Tech")
                with gr.Row():
                    image_style = gr.Dropdown(IMAGE_STYLES, label="üé® Image Style", value="Digital Art")
                    image_quality = gr.Radio(["standard", "detailed"], label="Image Quality", value="standard")
                gr.Markdown("#### üé• Video Settings")
                with gr.Row():
                    num_frames = gr.Slider(minimum=8, maximum=30, step=1, label="Number of Frames", value=14)
                    seconds = gr.Slider(minimum=2, maximum=10, step=0.5, label="Duration (Seconds)", value=4)
                with gr.Row():
                    motion_intensity = gr.Dropdown(["Low", "Medium", "High"], label="Motion Intensity", value="Medium")
                    video_style = gr.Dropdown(VIDEO_STYLES, label="Video Style", value="Cinematic")
            generate_btn = gr.Button("‚ú® Generate Content", variant="primary", elem_classes="generate-btn")

        with gr.Column(scale=1, elem_classes="output-section"):
            gr.Markdown("### üéâ Generated Content")
            text_output = gr.Textbox(label="üìù Post Content", interactive=True, lines=6)
            image_output = gr.Image(label="üñºÔ∏è Generated Image", height=500)
            video_output = gr.Video(label="üé• Generated Video", height=500)
            metrics_output = gr.Textbox(label="üìä Evaluation Metrics", interactive=False, lines=10)

    examples = gr.Examples(
        examples=[
            ["AI-powered resume builder", "Tech Launch", "LinkedIn", "Professional",
             "Mistral-7B (Recommended)", "Stable Diffusion XL (High Quality)", "Stable Video Diffusion (High Quality)",
             14, 4, "Medium", "Cinematic"],
            ["Summer sale on eco-friendly products", "Bold Sale", "Instagram", "Exciting",
             "OpenChat-3.5", "Stable Diffusion v1.5 (Balanced)", "Stable Video Diffusion (High Quality)",
             20, 5, "High", "Dynamic"],
            ["Time management techniques", "Tips & Tricks", "Twitter", "Friendly",
             "Phi-2 (Lightweight)", "Stable Diffusion v1.4 (Lightweight)", "Lightweight Video Model (Fallback)",
             10, 3, "Low", "Smooth Animation"]
        ],
        inputs=[prompt, template_type, platform, tone, selected_text_model, selected_image_model, selected_video_model,
                num_frames, seconds, motion_intensity, video_style],
        label="üí° Try these examples"
    )

    include_hashtags.change(
        lambda x: gr.Dropdown(visible=x),
        inputs=[include_hashtags],
        outputs=[hashtag_category]
    )

    generate_btn.click(
        generate_social_post,
        inputs=[prompt, template_type, platform, tone, include_hashtags, hashtag_category, image_style, image_quality,
                additional_info, selected_text_model, selected_image_model, selected_video_model, num_frames, seconds,
                motion_intensity, video_style],
        outputs=[text_output, image_output, video_output, metrics_output]
    )

app.launch(debug=True, share=True)
